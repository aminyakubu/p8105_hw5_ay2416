---
title: "Homework 5"
author: "Amin Yakubu"
date: "11/6/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(plotly)
```


### Problem 1 

Extracting the filepaths and names into lists
```{r}

paths_and_names = tibble(filepath = list.files("./data/", pattern = "*.csv", full.names = TRUE),
                         filename = basename(filepath))
```

Writing function to read the data

```{r}
read_fx = function(data){
  
  read_csv(file = data)
  
}

```

Now iterating using `map` function to read all the csv files and cleaning the data

```{r}

df = paths_and_names %>% mutate(data =  purrr::map(paths_and_names$filepath, read_fx)) %>% 
  unnest() %>% 
  select(-filepath) %>% 
  gather(key = week, value = value, week_1:week_8) %>% 
  mutate(id = str_replace(filename, ".csv",""),
         week = as.numeric(str_replace(week, "week_", ""))) %>% 
  separate(id, into = c("group", "id"), sep = "_") %>% 
  mutate(id = as.factor(id))

ggplot(df, aes(x = week, y = value, color = id, group = id)) + geom_point() + geom_line() + facet_grid(~group) +
  theme(legend.position = "bottom")
```

## Problem 2

```{r}
hom_df = read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv", col_names = TRUE) %>% 
  mutate(city_state = str_c(city, ",", " ", state))
```

Describe dataset ---

Summary statistics

Summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

```{r}
summ_df = hom_df %>% 
  mutate(disposition = fct_collapse(hom_df$disposition, "No arrest" = c("Closed without arrest","Open/No arrest"))) %>% group_by(city_state) %>% 
  count(disposition) %>% 
  spread(key = disposition, value = n) %>% 
  janitor::clean_names() %>% 
  mutate(total = closed_by_arrest + no_arrest)

```

For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

```{r}
baltimore = summ_df %>% filter(city_state == "Baltimore, MD")

prop.test(baltimore$no_arrest, baltimore$total) %>% 
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high) %>% 
  janitor::clean_names() %>% 
  knitr::kable()
```

Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and  unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

Writing a function for `prop.test` and cleaning data

```{r}
prop_test = function(data_table){
  
  try(prop.test(data_table$no_arrest, data_table$total) %>% 
    broom::tidy() %>% 
    select(estimate, conf.low, conf.high), silent = TRUE)
}

```

Iteration

```{r}
unnested_ci_for_states = summ_df %>% group_by(city_state) %>% nest() %>% 
  mutate(estimate_ci = map(data, prop_test)) %>% filter(city_state != "Tulsa, AL") %>% unnest() %>% 
  mutate(city_state = reorder(city_state, estimate)) %>% janitor::clean_names()
```

Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.

```{r}

ggplot(unnested_ci_for_states, aes(x = city_state, y = estimate )) + 
  geom_point() + geom_errorbar(aes(ymin = conf_low, ymax = conf_high)) + 
  theme(axis.text.x = element_text(angle = 80, hjust = 1, size = 8)) +
  labs(
    x = "City, State",
    y = "Proportion (95% Confidence interval)"
  )

```





